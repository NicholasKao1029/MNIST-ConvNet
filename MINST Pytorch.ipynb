{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- MNIST dataset \n",
    "    - hand drawn digits \n",
    "    - black and white images\n",
    "    - NN's on this dataset are considered the \"hello world\" of NN\n",
    "    - each image is a 28 X 28 pixel image = 784 pixels\n",
    "    - each pixel corresponds to an input in the input layer\n",
    "    - each pixel has a value of 0-255 traditionally, but is normalized to fit in between a value of 0-1\n",
    "    - each image has a corresponding label that identifies which digit out of the 10 possible it represents\n",
    "    \n",
    "    \n",
    "- Using Pytorch already cleaned MNIST dataset, have my neural networ try to learn to recognize hand written digits and classify them as accurately as possible.  \n",
    "\n",
    "### Splitting data \n",
    "\n",
    "- Neural Net's learn from the data that is fed to them, they essentially try to minimize a cost function which is based on the outputs it's predicts and the actual values that are correct.\n",
    "\n",
    "- So to make sure that our neural net is learning to be able to generalize across hand written digits instead of \"overfitting\" and simply learning the particular dataset it's being trained on. The data that we have must be split into two groups (can be 3, I believe, but 2 is neccessary). \n",
    "    - Training data, usually 70%-80% of the entire data set. This is the data that will be used to learn from\n",
    "    - Testing data, seperate from the training data, this set will be used to evaluate how well our neural network is performing on \"out of sample data\" or data it hasn't seen yet. True indicator of ability of recognition.\n",
    "   \n",
    "### Data cleanliness\n",
    "\n",
    "- This is very unrealistic of real world data, this data has been manipulated to be in a form that is perfect to use with PyTorch and can be used essentially immediately. Much of the work and time you put in will be making real world data usable, transform into a form that will allow to conduct learning from. But this is a good starting point.\n",
    "\n",
    "- Another point to think about is the fact that if our data has e.g. 55% 3's and 5% for each of the other digits, the quickest way to decrease lost generally speaking is to guess 3 more of the time, which is a terrible thing for generlization but great to reduce cost based on the data set. Which is how the NN determines how to change the parameters of the model.\n",
    "    - This is an issue of BALANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test =  datasets.MNIST(\"\", train=False, download=True, transform = transforms.Compose([transforms.ToTensor()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchSize and Shuffle \n",
    "\n",
    "### BatchSize\n",
    "\n",
    "- This particular dataset is not particularly large in terms of the memory on disk, however this is not always the case and in general when trying to learn new things, the larger our data set the better it is for our neural net to learn. So BatchSize is the size that we will use to determine Gradient Descent (GD). \n",
    "    - There is a trade off occuring here\n",
    "        - Basically the batch will determine the way to change our weights and biases as to minimize our cost function.\n",
    "        - Since it is only a small sample out of the entire data set, it is not guarenteed or even likely that the change will minimize the cost of the entire sample perfectl. However it will probably over many iterations give a pretty good direction to the minimum of the cost function, while not needing to process the entire data set. So the time gained from not needing to process the entire data set will pay for the fact we are not optimizing our change of weights to minimize the cost function.\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "### Shuffle \n",
    "\n",
    "- The MNIST data set could have each of the samples grouped based on the classification, e.g. all the 2's are together then 4's etc. As a result if our neural net is trained with the samples in this particular order. It will optimize at first to classify all digits as a 2 because that's all it has ever seen and known. Then when the 2's are exhausted and 4's show up it will have to radically change the weights, as it had no reason not to classify everything it saw as a 2. \n",
    "\n",
    "- Our goal is to have our Neural network be able to generally learn to recognize hand written digits. So we should try and help it as much as possible, because it is basically just trying to minimize a cost function and if that function is basically saying call everything a 2 it will do that. \n",
    "\n",
    "- Shuffle allows it to see a wider range of the data set, spread out better. Because the model doesn't know how good it can get and will not think ahead, instead it will simply try to decrease the cost as much as possible right now based on what it knows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([8, 5, 3, 3, 1, 8, 9, 8, 4, 3])]\n"
     ]
    }
   ],
   "source": [
    "print(type(trainset))\n",
    "\n",
    "\"\"\"\n",
    "trainset is a DataLoader\n",
    "data are 2 Tensors\n",
    "\n",
    "First Tensor is all of the \"raw data\", the 2nd and last tensor are the labels corresponding to the data in the first tensor\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n"
     ]
    }
   ],
   "source": [
    "x,y = data[0][0], data[1][0]\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#above we see that data[0][0] has shape ([1,28,28]), which is a rank 3 tensor, so if we want to plto this and see how it \n",
    "#looks on matplotlib we have to change it's shape using view() to a \"2d format\"  \n",
    "\n",
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "# verifying balance in data set\n",
    "\n",
    "total = 0 \n",
    "dict = {0:0, \n",
    "        1:0, \n",
    "        2:0,  \n",
    "        3:0, \n",
    "        4:0, \n",
    "        5:0, \n",
    "        6:0, \n",
    "        7:0, \n",
    "        8:0, \n",
    "        9:0, \n",
    "       }\n",
    "\n",
    "\n",
    "for data in trainset: \n",
    "    for d in data[1]:\n",
    "        dict[int(d)] += 1\n",
    "        total += 1\n",
    "\n",
    "print(dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: percentage of 9.87%\n",
      "1: percentage of 11.24%\n",
      "2: percentage of 9.93%\n",
      "3: percentage of 10.22%\n",
      "4: percentage of 9.74%\n",
      "5: percentage of 9.04%\n",
      "6: percentage of 9.86%\n",
      "7: percentage of 10.44%\n",
      "8: percentage of 9.75%\n",
      "9: percentage of 9.92%\n"
     ]
    }
   ],
   "source": [
    "for i in dict:\n",
    "    print(\"{}: percentage of {:.2f}%\".format(i,dict[i]/total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #nn.Linear(input, output)\n",
    "        #flatten the image into a column vector basically\n",
    "        #first layer takes in the input which is the 28^2=784 pixels\n",
    "        #the last layer which is fc4 must output 10 for the 10 possible digits\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        \n",
    "        # relu Rectified linear unit\n",
    "        # activation function\n",
    "        \n",
    "        # softmax takes in the output vector and normalizes it to a probability distribution\n",
    "        # limits the output to 0 - 1 range\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim = 1)\n",
    "        \n",
    "\n",
    "        \n",
    "net = Net()\n",
    "print(net)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(28,28)\n",
    "\n",
    "# the -1 indicates that it can be an array of any size? interesting \n",
    "\n",
    "X = X.view(-1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1557, -2.4043, -2.2592, -2.2558, -2.3653, -2.2763, -2.2992, -2.4266,\n",
       "         -2.3782, -2.2382]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9776e-05, grad_fn=<NllLossBackward>)\n",
      "tensor(0., grad_fn=<NllLossBackward>)\n",
      "tensor(3.5763e-08, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3842e-08, grad_fn=<NllLossBackward>)\n",
      "Time elapsed: 103.073 sec\n"
     ]
    }
   ],
   "source": [
    "# net.parameters talk about what is adjustable\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0001)\n",
    "\n",
    "EPOCHS = 5\n",
    "start = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        # data is a batch of featuresets and labels\n",
    "        X, y = data\n",
    "        net.zero_grad() # batches hep to generali\n",
    "        output = net(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "end = time.time()\n",
    "print(\"Time elapsed: {} sec\".format(round(end - start, 3)));\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9796\n"
     ]
    }
   ],
   "source": [
    "correct = 0 \n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1 \n",
    "            total += 1\n",
    "\n",
    "print(\"Accuracy: \", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "\n",
      "\n",
      "PREDICTION\n",
      "\n",
      "\n",
      "tensor(3, grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()\n",
    "\n",
    "print(y[0])\n",
    "print(\"\\n\")\n",
    "print(\"PREDICTION\")\n",
    "print(\"\\n\")\n",
    "print(torch.argmax(net(X[0].view(-1,784))[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff hyper params,\n",
    "\n",
    "- Epoch = 5, LR = 0.0001\n",
    "    - Accuracy: 0.9551\n",
    "    - Time: 97.705 sec\n",
    "    - loss over epoch\n",
    "        - tensor(0.1222, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0.8047, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0.1421, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0.0839, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0.0054, grad_fn=<NllLossBackward>)\n",
    "\n",
    "- Epoch = 5, LR = 0.0001\n",
    "    - Accuracy: 0.9796\n",
    "    - Time: 103.073\n",
    "    - loss over epoch\n",
    "        - tensor(0.0009, grad_fn=<NllLossBackward>)\n",
    "        - tensor(4.9776e-05, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0., grad_fn=<NllLossBackward>)\n",
    "        - tensor(3.5763e-08, grad_fn=<NllLossBackward>)\n",
    "        - tensor(2.3842e-08, grad_fn=<NllLossBackward>)\n",
    "  \n",
    "- Epoch = 5, LR = 0.001\n",
    "    - Accuracy: 0.9719\n",
    "    - Time: 99.416 sec\n",
    "    - loss over epoch\n",
    "        - tensor(0.0303, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0.7818, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0.0120, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0.0207, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0.0039, grad_fn=<NllLossBackward>)\n",
    "\n",
    "- Epoch = 3, LR = 0.001\n",
    "    - Accuracy: 0.9742\n",
    "    - Time: 59.715 sec\n",
    "    - loss over epoch\n",
    "        - tensor(0.0018, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0.0908, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0.0017, grad_fn=<NllLossBackward>)\n",
    "    \n",
    "- Epoch = 3, LR = 0.0001\n",
    "    - Accuracy: 0.9789\n",
    "    - Time: 58.7 sec\n",
    "    - loss over epoch\n",
    "        - tensor(2.5272e-06, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0.0005, grad_fn=<NllLossBackward>)\n",
    "        - tensor(0.0015, grad_fn=<NllLossBackward>)\n",
    "    \n",
    "\n",
    "- Seems very luck based with multiple tries with the same params, sometimes the NN find's it's minimum extremely fast, sometimes it pops out of the minimum even with a low LR, in one of the epochs, just have to hope it isn't the last I suppose. the time gains are interesting. \n",
    "    \n",
    "- Timing wise, O(E) makes sense this was done on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
